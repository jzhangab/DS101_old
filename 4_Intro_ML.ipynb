{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "## Identifying the causes of Diabetes\n",
    "In this notebook you will implement your first machine learning algorithm to analyze a population health dataset: the Pima Indian diabetes dataset. The purpose of this analysis is to learn how to utilize machine learning to solve a very specific problem - identifying individuals at risk of diabetes and discovering potential causes of Diabetes in the population.\n",
    "\n",
    "## Contents\n",
    "1. Import dataset\n",
    "2. Data exploration\n",
    "3. Feature engineering\n",
    "4. Modeling\n",
    "5. Model Evaluation\n",
    "\n",
    "## How to use this notebook\n",
    "- To execute any single block of text or markdown, use ctrl+enter, shift+enter or press the run arrow on the left of the box (only in Colaboratory)\n",
    "- To reset the notebook select \"Factory reset runtime\" from the Runtime tab at the top of Colaboratory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's import our data\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/jzhangab/DS101/master/1_Data/diabetes.csv'\n",
    "df = pd.read_csv(url, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first 5 rows to begin understanding what factors are available\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data frame we can see that the dataset consists of 8 different factors that contribute to the risk of diabetes. The actual truth of whether or not an individual has diabetes is in the column \"Outcome\". We will use this information to train a machine learning model to understand how the different factors are connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "The purpose of data exploration is to seek to understand the data. We will look primarily at histograms and scatterplots to visualize if there are any interesting relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the histograms of the dataframe to understand each factor.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "ax = fig.gca()\n",
    "df.hist(ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age is skewed young, but how does it relate to one of the more normally distributed factors such as BloodPressure?\n",
    "# Try changing the x and y variables in the scatterplot declaration to view other relationships.\n",
    "%matplotlib inline\n",
    "df.plot.scatter(x = 'Age',\n",
    "                y = 'BloodPressure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about BMI vs. Outcome? Are higher BMI persons more likely to have diabetes?\n",
    "# This shows that you may be tempted to draw conclusions using single factor analysis that BMI causes diabetes - when it is only one of several factors\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = pd.DataFrame(df['BMI'])\n",
    "y = pd.DataFrame(df['Outcome'])\n",
    "\n",
    "# create a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "# predict y from the data\n",
    "x_new = np.linspace(15, 60, 100)\n",
    "y_new = model.predict(x_new[:, np.newaxis])\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = plt.axes()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x_new, y_new)\n",
    "\n",
    "ax.set_xlabel('BMI')\n",
    "ax.set_ylabel('Outcome')\n",
    "\n",
    "ax.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "The purpose of feature engineering is to prepare data for modeling. The diabetes data set is formatted well and does not contain text variables so we will only do two things to prepare the data\n",
    "\n",
    "1. Missing data\n",
    "2. Reduce multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By far the most important thing to understand about a dataset is how \"clean\" it might be\n",
    "# For cleanliness, missing data is very important, let's check how much missing data there is for each factor\n",
    "for col in list(df):\n",
    "    num_na = len(df[col]) - df[col].count()\n",
    "    print (\"Percent null in column \" + col + \" is:\", 100*num_na/len(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some columns such as Glucose, BloodPressure, SkinThickness, Insulin, BMI, and Age we are not only concerned with null values but also 0 values\n",
    "for col in list(df):\n",
    "    num_0 = len(df.loc[df[col] == 0][col])\n",
    "    print (\"Percent 0 in column \" + col + \" is:\", 100*num_0/len(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the following to clean our data\n",
    "\n",
    "# 1. Replace all null values with 0\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# 2. Remove any data point where any of the following are 0: Glucose, BloodPressure, SkinThickness, Insulin, BMI, or Age\n",
    "# List of factors we want to remove 0 values from\n",
    "nonzero_factors = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI'] \n",
    "# Iterate over the list of columns and subset the main dataframe by each column where values are nonzero\n",
    "for col in nonzero_factors:\n",
    "    df = df.loc[df[col] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity\n",
    "The idea of collinearity is that if certain input factors are closely correlated, they will bias the output of the model by amplifying their particular effects. We need to understand if some of our factors are high collinear and then reduce bias by removing all but 1 of the collinear factors from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check the correlation (R-square) between variables using a correlation matrix\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To quantify multicollinearity, we will use variance inflation factor (VIF)\n",
    "# Rule of thumb, VIF above 10 indicates a particular variable ought to be removed\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# Also - VIF for a constant term should be high because the intercept is a proxy for the constant.\n",
    "# A constant term needs to be added to accurately measure VIF for the other terms\n",
    "df_c = add_constant(df[[c for c in list(df) if 'PARAM' in c]])\n",
    "\n",
    "# inline Generator on a pandas series\n",
    "pd.Series([variance_inflation_factor(df_c.values, i) \n",
    "               for i in range(df_c.shape[1])], \n",
    "              index=df_c.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling\n",
    "In the modeling step we will train a supervised machine learning model to understand relationships in the diabetes data set. We will then evaluate the model to see how well it predicts.\n",
    "\n",
    "The particular model that we will use is Logistic Regression. This model is commonly used in binary classification for predictive analytics.\n",
    "\n",
    "1. Split dataset into training and validation datasets\n",
    "2. Train model\n",
    "3. Predict outcomes of validation dataset\n",
    "4. Calculate accuracy of validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will split the data 80%/20% using 80% of the data to train the model and 20% to validate the accuracy of the model\n",
    "# We can use pre-built functions from the machine learning package sci-kit learn to do this task\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# The Outcome column not part of input features so we will use a generator to create a new list and call it \"features\"\n",
    "features = [col for col in list(df) if col not in ['Outcome']]\n",
    "\n",
    "# The X input is df[features] which is all columns in the dataframe of the list features we created using the generator\n",
    "# The Y input is df['Outcome'] which is the binary label column\n",
    "X = df[features]\n",
    "y = df['Outcome']\n",
    "\n",
    "# Generate the 4 datasets we need\n",
    "# X_train and y_train to train the model\n",
    "# X_test to generate predictions\n",
    "# y_test to evaluate the accuracy of the predictions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok so what's the difference between X_train and X_test???\n",
    "print(\"Length of X_train \", len(X_train))\n",
    "print(\"Length of X_test \", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare and fit model\n",
    "model = LogisticRegression(random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "We will use several techniques to evaluate the strength of the Model\n",
    "\n",
    "1. Accuracy\n",
    "2. Confusion Matrix (false positive, true positive, false negative, true negative)\n",
    "3. Receiver operating characteristic\n",
    "4. Sigmoid probability visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare y_test (true values) to y_pred (predicted values)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the confusion matrix, which shows us false positives and false negatives\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method of evaluating a classifier is using the Receiver Operating Characteristic (ROC)\n",
    "# ROC is a plot of true positive vs. false positive. We calculate the area under the curve (AUC)\n",
    "# AUC = 1 indicates a perfect classifier, AUC = 0.5 means the classifier is no better than a coin flip\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "%matplotlib inline\n",
    "y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "falseposrate, trueposrate, _ = roc_curve(y_test, y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(falseposrate,trueposrate,label=\"ROC curve, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Logistic Regression it is possible to use a Sigmoid curve to visualize the probability function for each variable\n",
    "# Below you will see the Sigmoid probability function for a single variable. Where the Outcome is greater than 0.5, the model is more likely to predict that data point as a 1 (Positive for diabetes)\n",
    "# Try changing the x variable to visualize this decision function for each variable\n",
    "\n",
    "import seaborn as sns\n",
    "sns.regplot(x='Glucose', y='Outcome', data=df, logistic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
